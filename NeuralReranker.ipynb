{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7-Mec8NuJST",
        "outputId": "59fe8165-16d5-4191-cbd0-4080b4aa7bb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.17.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.22.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "# Install PyTorch with CUDA 11.8\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# Install the rest\n",
        "!pip install numpy pandas tqdm transformers scikit-learn datasets spacy sentence-transformers nltk\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# Uncomment these commands if you want a fresh run:\n",
        "#!rm -rf stage3_cache_transformer_ner/\n",
        "#!rm -f preprocessed_hotpotqa_stable.parquet\n",
        "#!rm -rf ./models/neural_ranker_final_hotpotqa/\n",
        "print(\"All old cache files have been deleted.\")\n",
        "\"\"\"\n",
        "Neural Ranker Training Script for Multi-Hop RAG Pipeline\n",
        "Trains a cross-encoder model on HotpotQA dataset with pipeline-specific features.\n",
        "This version uses a high-performance, GPU-accelerated Transformer for NER and includes\n",
        "robust, transactional caching and error handling.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import re\n",
        "import multiprocessing as mp\n",
        "from multiprocessing import Pool\n",
        "import shutil\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import json\n",
        "import logging\n",
        "from typing import List, Dict, Tuple, Optional, Any\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSequenceClassification,\n",
        "    TrainingArguments, Trainer, EarlyStoppingCallback,\n",
        "    pipeline as hf_pipeline\n",
        ")\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Ensure NLTK data is available\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ==============================================================================\n",
        "# --- CONFIGURATION ---\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    model_name: str = \"microsoft/deberta-v3-base\"\n",
        "    max_length: int = 512\n",
        "    batch_size: int = 12\n",
        "    learning_rate: float = 1.5e-5\n",
        "    num_epochs: int = 4\n",
        "\n",
        "    warmup_ratio: float = 0.20\n",
        "    weight_decay: float = 0.05\n",
        "    hard_negative_ratio: float = 0.1 # Crucial for preventing trivial solutions\n",
        "\n",
        "    output_dir: str = \"./models/neural_ranker\"\n",
        "    save_steps: int = 250\n",
        "    eval_steps: int = 250\n",
        "    logging_steps: int = 100\n",
        "    load_best_model_at_end: bool = True\n",
        "    metric_for_best_model: str = \"eval_f1\"\n",
        "    greater_is_better: bool = True\n",
        "    dataloader_num_workers: int = 2\n",
        "    fp16: bool = torch.cuda.is_available()\n",
        "    gradient_accumulation_steps: int = 3\n",
        "    max_grad_norm: float = 1.0\n",
        "\n",
        "    # Data processing features\n",
        "    min_sentence_length: int = 20\n",
        "    use_hop_features: bool = True\n",
        "    use_rhetorical_features: bool = True\n",
        "    use_entity_features: bool = True\n",
        "    use_reasoning_features: bool = True\n",
        "    embedding_batch_size: int = 128\n",
        "    ner_batch_size: int = 64\n",
        "\n",
        "@dataclass\n",
        "class SentenceWithMetadata:\n",
        "    text: str; doc_title: str; doc_id: int; sent_id: int; entities: List[str]\n",
        "    rhetorical_role: str = \"Background\"; hop_depth: int = 0\n",
        "    has_reasoning_signals: bool = False; is_causal: bool = False\n",
        "\n",
        "# ==============================================================================\n",
        "# --- HIGH-PERFORMANCE DATA PROCESSOR ---\n",
        "# ==============================================================================\n",
        "class HotpotQADataProcessor:\n",
        "    rhetorical_patterns = {\n",
        "        'Main Claim': ['argue', 'claim', 'assert', 'believe', 'conclude'],\n",
        "        'Supporting Evidence': ['evidence', 'data', 'research', 'study', 'found'],\n",
        "        'Expert Opinion': ['expert', 'according to', 'stated', 'opinion'],\n",
        "    }\n",
        "\n",
        "    def __init__(self, config: TrainingConfig, sbert_model: SentenceTransformer):\n",
        "        self.config = config\n",
        "        self.sbert_model = sbert_model\n",
        "        if self.config.use_entity_features:\n",
        "            logger.info(\"Loading GPU-accelerated Transformer NER pipeline...\")\n",
        "            self.ner_pipeline = hf_pipeline(\n",
        "                \"ner\", model=\"dslim/bert-base-NER\", tokenizer=\"dslim/bert-base-NER\",\n",
        "                device=0 if torch.cuda.is_available() else -1, grouped_entities=True\n",
        "            )\n",
        "        else:\n",
        "            self.ner_pipeline = None\n",
        "\n",
        "    @staticmethod\n",
        "    def _initial_parse_worker(row: Dict) -> Optional[Dict]:\n",
        "        try:\n",
        "            query = row.get('query')\n",
        "            if not isinstance(query, str) or not query.strip(): return None\n",
        "            sp_titles = set(row.get('sp', []))\n",
        "            raw_context = row.get('context', \"\")\n",
        "            if not isinstance(raw_context, str) or not raw_context.strip(): return None\n",
        "            docs_raw = re.split(r'\\n\\nTitle: ', raw_context)\n",
        "            sentences = []\n",
        "            if docs_raw and docs_raw[0].strip():\n",
        "                first_doc = docs_raw[0].strip()\n",
        "                if not first_doc.startswith(\"Title:\"):\n",
        "                    docs_raw[0] = \"Title: \" + first_doc\n",
        "            for doc_part in docs_raw:\n",
        "                if not doc_part or not doc_part.strip(): continue\n",
        "                lines = doc_part.split('\\n', 1)\n",
        "                doc_title = lines[0].replace(\"Title: \", \"\").strip()\n",
        "                doc_text = lines[1].strip() if len(lines) > 1 else \"\"\n",
        "                for sent_text in sent_tokenize(doc_text):\n",
        "                    if len(sent_text) > TrainingConfig.min_sentence_length:\n",
        "                        sentences.append({\n",
        "                            \"text\": sent_text, \"doc_title\": doc_title,\n",
        "                            \"is_positive_source\": doc_title in sp_titles\n",
        "                        })\n",
        "            return {\"query\": query, \"sentences\": sentences}\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Critical error parsing row ID {row.get('query_id', 'N/A')}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _batch_process_sentences(self, all_sentences: List[Dict], chunk_size: int = 20000):\n",
        "        logger.info(f\"Batch processing {len(all_sentences)} unique sentences (chunk_size={chunk_size})...\")\n",
        "        sentence_texts = [s['text'] for s in all_sentences]\n",
        "        embeddings_list = []\n",
        "        try:\n",
        "            for i in tqdm(range(0, len(sentence_texts), chunk_size), desc=\"SBERT Encoding Chunks\"):\n",
        "                chunk = sentence_texts[i:i + chunk_size]\n",
        "                embeddings_list.append(self.sbert_model.encode(\n",
        "                    chunk, batch_size=self.config.embedding_batch_size,\n",
        "                    show_progress_bar=False, convert_to_numpy=True\n",
        "                ))\n",
        "            sentence_embeddings = np.vstack(embeddings_list) if embeddings_list else np.zeros((0, self.sbert_model.get_sentence_embedding_dimension()))\n",
        "        except Exception as e:\n",
        "            logger.error(f\"SBERT encoding failed: {e}\")\n",
        "            dim = self.sbert_model.get_sentence_embedding_dimension()\n",
        "            sentence_embeddings = np.zeros((len(sentence_texts), dim))\n",
        "        all_entities = [[] for _ in sentence_texts]\n",
        "        if self.ner_pipeline:\n",
        "            logger.info(\"Running Transformer NER batch processing...\")\n",
        "            for i in tqdm(range(0, len(sentence_texts), chunk_size), desc=\"Transformer NER Chunks\"):\n",
        "                chunk_texts = sentence_texts[i:i + chunk_size]\n",
        "                try:\n",
        "                    results = self.ner_pipeline(chunk_texts, batch_size=self.config.ner_batch_size)\n",
        "                    for j, ner_result in enumerate(results):\n",
        "                        all_entities[i + j] = [(entity['word'], entity['entity_group']) for entity in ner_result]\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"NER chunk failed for sentences {i} to {i+len(chunk_texts)}: {e}\")\n",
        "        return sentence_embeddings, all_entities\n",
        "\n",
        "    @staticmethod\n",
        "    def _assemble_examples_worker(args: Tuple) -> List[Tuple[str, int, Dict]]:\n",
        "        parsed_row, query_embedding, sentence_data_map, config, embedding_dim = args\n",
        "        query, sentences = parsed_row['query'], parsed_row['sentences']\n",
        "        if not sentences: return []\n",
        "        def _classify_rhetorical_role(text):\n",
        "            text_lower = text.lower()\n",
        "            for role, patterns in HotpotQADataProcessor.rhetorical_patterns.items():\n",
        "                if any(p in text_lower for p in patterns): return role\n",
        "            return \"Background_Information\"\n",
        "        def _detect_causal_language(text):\n",
        "            return any(word in text.lower() for word in ['cause', 'because', 'due to', 'result', 'lead to'])\n",
        "        def _create_structured_input(q, s_meta):\n",
        "            components = []\n",
        "            if config.use_hop_features: components.append(f\"[HOP:{s_meta.hop_depth}]\")\n",
        "            if config.use_rhetorical_features: components.append(f\"[ROLE:{s_meta.rhetorical_role.replace(' ', '_')}]\")\n",
        "            if config.use_entity_features and s_meta.entities: components.append(f\"[ENT:{','.join(s_meta.entities[:3])}]\")\n",
        "            if config.use_reasoning_features:\n",
        "                if s_meta.is_causal: components.append(\"[CAUSAL]\")\n",
        "                if s_meta.has_reasoning_signals: components.append(\"[REASONING]\")\n",
        "            return f\"{' '.join(components)} {q} [SEP] {s_meta.text}\"\n",
        "        positives, negatives = [], []\n",
        "        sent_emb_list = [sentence_data_map.get(s['text'], {}).get('embedding', np.zeros(embedding_dim)) for s in sentences]\n",
        "        sentence_embeddings = np.array(sent_emb_list)\n",
        "        q_norm = np.linalg.norm(query_embedding) + 1e-9\n",
        "        s_norms = np.linalg.norm(sentence_embeddings, axis=1) + 1e-9\n",
        "        similarities = (sentence_embeddings @ query_embedding) / (s_norms * q_norm)\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            sent_text = sentence['text']\n",
        "            entry = sentence_data_map.get(sent_text, {})\n",
        "            entities = entry.get('entities', [])\n",
        "            is_causal = _detect_causal_language(sent_text)\n",
        "            sentence_meta = SentenceWithMetadata(\n",
        "                text=sent_text, doc_title=sentence.get('doc_title', ''),\n",
        "                entities=[e[0] for e in entities], rhetorical_role=_classify_rhetorical_role(sent_text),\n",
        "                is_causal=is_causal, has_reasoning_signals=(len(entities) > 1 or is_causal),\n",
        "                doc_id=sentence.get('doc_id', 0), sent_id=sentence.get('sent_idx', i)\n",
        "            )\n",
        "            structured_input = _create_structured_input(query, sentence_meta)\n",
        "            metadata = {'similarity': float(similarities[i]), 'doc_title': sentence.get('doc_title', '')}\n",
        "            example = (structured_input, int(sentence.get('is_positive_source', False)), metadata)\n",
        "            if sentence.get('is_positive_source', False): positives.append(example)\n",
        "            else: negatives.append(example)\n",
        "        if not positives: return []\n",
        "        negatives.sort(key=lambda x: x[2]['similarity'], reverse=True)\n",
        "        num_hard = int(len(positives) * config.hard_negative_ratio)\n",
        "        selected_negatives = negatives[:num_hard] + negatives[-(len(positives) - num_hard):]\n",
        "        return positives + selected_negatives\n",
        "\n",
        "    def process_dataset(self, dataset: List[Dict], num_workers: int) -> List[Tuple[str, int, Dict]]:\n",
        "        cache_dir = \"stage3_cache_transformer_ner\"\n",
        "        os.makedirs(cache_dir, exist_ok=True)\n",
        "        cache_files = {\"parsed_rows.json\", \"sentence_embeddings.npy\", \"sentence_texts.json\", \"sentence_meta.json\", \"query_embeddings.npy\", \"query_texts.json\"}\n",
        "        if all(os.path.exists(os.path.join(cache_dir, f)) for f in cache_files):\n",
        "            logger.info(\"Loading Stage 3 cache to accelerate processing...\")\n",
        "            with open(os.path.join(cache_dir, \"parsed_rows.json\"), \"r\") as f: parsed_rows = json.load(f)\n",
        "            sentence_embeddings = np.load(os.path.join(cache_dir, \"sentence_embeddings.npy\"))\n",
        "            with open(os.path.join(cache_dir, \"sentence_texts.json\"), \"r\") as f: sentence_texts = json.load(f)\n",
        "            with open(os.path.join(cache_dir, \"sentence_meta.json\"), \"r\") as f: sentence_meta = json.load(f)\n",
        "            query_embeddings = np.load(os.path.join(cache_dir, \"query_embeddings.npy\"))\n",
        "            with open(os.path.join(cache_dir, \"query_texts.json\"), \"r\") as f: query_texts = json.load(f)\n",
        "            query_embedding_map = {q: emb for q, emb in zip(query_texts, query_embeddings)}\n",
        "            sentence_data_map = {text: {\"embedding\": sentence_embeddings[i], \"entities\": meta.get(\"entities\", [])} for i, (text, meta) in enumerate(zip(sentence_texts, sentence_meta))}\n",
        "        else:\n",
        "            logger.info(\"No complete cache found. Running full preprocessing pipeline...\")\n",
        "            with Pool(processes=num_workers) as pool:\n",
        "                parsed_rows = list(tqdm(pool.imap(self._initial_parse_worker, dataset), total=len(dataset), desc=\"Stage 1: Parsing Rows\"))\n",
        "            parsed_rows = [row for row in parsed_rows if row and row.get('sentences')]\n",
        "            if not parsed_rows: raise ValueError(\"Stage 1 parsing resulted in zero processable rows.\")\n",
        "            unique_sentences = {s['text']: s for row in parsed_rows for s in row['sentences']}\n",
        "            unique_queries = sorted(list({row['query'] for row in parsed_rows}))\n",
        "            all_unique_sentences = list(unique_sentences.values())\n",
        "            sentence_embeddings, all_entities = self._batch_process_sentences(all_unique_sentences)\n",
        "            query_embeddings = self.sbert_model.encode([f\"query: {q}\" for q in unique_queries], batch_size=self.config.embedding_batch_size, show_progress_bar=True, convert_to_numpy=True)\n",
        "            query_embedding_map = {q: emb for q, emb in zip(unique_queries, query_embeddings)}\n",
        "            sentence_texts = [s['text'] for s in all_unique_sentences]\n",
        "            sentence_meta = [{\"entities\": all_entities[i]} for i in range(len(all_entities))]\n",
        "            sentence_data_map = {text: {\"embedding\": sentence_embeddings[i], \"entities\": sentence_meta[i][\"entities\"]} for i, text in enumerate(sentence_texts)}\n",
        "            temp_files = {f: f\"{f}.tmp\" for f in cache_files} # Use .tmp extension\n",
        "            try:\n",
        "                with open(os.path.join(cache_dir, temp_files[\"parsed_rows.json\"]), \"w\") as f: json.dump(parsed_rows, f)\n",
        "                np.save(os.path.join(cache_dir, temp_files[\"sentence_embeddings.npy\"]), sentence_embeddings)\n",
        "                with open(os.path.join(cache_dir, temp_files[\"sentence_texts.json\"]), \"w\") as f: json.dump(sentence_texts, f)\n",
        "                with open(os.path.join(cache_dir, temp_files[\"sentence_meta.json\"]), \"w\") as f: json.dump(sentence_meta, f)\n",
        "                np.save(os.path.join(cache_dir, temp_files[\"query_embeddings.npy\"]), query_embeddings)\n",
        "                with open(os.path.join(cache_dir, temp_files[\"query_texts.json\"]), \"w\") as f: json.dump(unique_queries, f)\n",
        "                for final, temp in temp_files.items():\n",
        "                    shutil.move(os.path.join(cache_dir, temp), os.path.join(cache_dir, final))\n",
        "                logger.info(f\"Saved Stage 3 cache to '{cache_dir}'.\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Failed to write cache files: {e}. Cleaning up.\")\n",
        "                for temp in temp_files.values():\n",
        "                    if os.path.exists(os.path.join(cache_dir, temp)): os.remove(os.path.join(cache_dir, temp))\n",
        "\n",
        "        logger.info(\"Stage 4: Assembling final training examples using threads...\")\n",
        "        all_examples = []\n",
        "        assembly_args = [(row, query_embedding_map[row['query']], sentence_data_map, self.config, sentence_embeddings.shape[1]) for row in parsed_rows]\n",
        "        with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "            futures = [executor.submit(self._assemble_examples_worker, arg) for arg in assembly_args]\n",
        "            for future in tqdm(as_completed(futures), total=len(futures), desc=\"Stage 4: Assembling Examples\"):\n",
        "                all_examples.extend(future.result())\n",
        "        return all_examples\n",
        "\n",
        "# ==============================================================================\n",
        "# --- DATASET & TRAINER CLASSES ---\n",
        "# ==============================================================================\n",
        "class RankerDataset(Dataset):\n",
        "    def __init__(self, examples: List[Tuple[str, int, Dict]], tokenizer, max_length: int):\n",
        "        self.examples = examples; self.tokenizer = tokenizer; self.max_length = max_length\n",
        "    def __len__(self): return len(self.examples)\n",
        "    def __getitem__(self, idx):\n",
        "        text, label, _ = self.examples[idx]\n",
        "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
        "        return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'labels': torch.tensor(label, dtype=torch.float)}\n",
        "\n",
        "class WeightedTrainer(Trainer):\n",
        "    \"\"\"Custom Trainer to apply a weight to the positive class in the loss function.\"\"\"\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "        # Penalize the model more for misclassifying a \"0\" than a \"1\".\n",
        "        pos_weight = torch.tensor([1.2], device=model.device) # 20% more penalty for false negatives\n",
        "        loss_fct = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "        loss = loss_fct(logits.view(-1), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "class NeuralRankerTrainer:\n",
        "    def __init__(self, config: TrainingConfig):\n",
        "        self.config = config\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
        "        self.sbert_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.data_processor = HotpotQADataProcessor(config, self.sbert_model)\n",
        "        special_tokens = ['[HOP:0]', '[HOP:1]', '[ROLE:Main_Claim]', '[ROLE:Supporting_Evidence]', '[ROLE:Expert_Opinion]', '[ROLE:Background_Information]', '[ENT:', '[CAUSAL]', '[REASONING]']\n",
        "        new_tokens = [token for token in special_tokens if token not in self.tokenizer.vocab]\n",
        "        if new_tokens:\n",
        "            self.tokenizer.add_special_tokens({'additional_special_tokens': new_tokens})\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(config.model_name, num_labels=1, problem_type=\"regression\")\n",
        "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
        "        logger.info(f\"Initialized model: {config.model_name}\")\n",
        "\n",
        "    def prepare_datasets(self, dataset_name: str, subset: str, train_split: str, max_train_samples: int, cache_file: str, num_workers: int):\n",
        "        if os.path.exists(cache_file):\n",
        "            logger.info(f\"Loading preprocessed dataset from {cache_file}...\")\n",
        "            df = pd.read_parquet(cache_file)\n",
        "            all_examples = [(row['text'], int(row['label']), json.loads(row['metadata'])) for _, row in df.iterrows()]\n",
        "        else:\n",
        "            logger.info(f\"Loading dataset: {dataset_name}, subset: {subset}\")\n",
        "            dataset = load_dataset(dataset_name, name=subset, split=train_split)\n",
        "            if max_train_samples:\n",
        "                dataset = dataset.select(range(min(max_train_samples, len(dataset))))\n",
        "            all_examples = self.data_processor.process_dataset(list(dataset), num_workers=num_workers)\n",
        "            if not all_examples: raise RuntimeError(\"Data processing yielded zero examples. Aborting.\")\n",
        "            df_to_cache = pd.DataFrame([{\"text\": t, \"label\": l, \"metadata\": json.dumps(m)} for t, l, m in all_examples])\n",
        "            df_to_cache.to_parquet(cache_file, index=False)\n",
        "            logger.info(f\"Saved preprocessed data to {cache_file}\")\n",
        "        np.random.shuffle(all_examples)\n",
        "        split_point = int(0.9 * len(all_examples))\n",
        "        train_examples, eval_examples = all_examples[:split_point], all_examples[split_point:]\n",
        "        logger.info(f\"Total examples: {len(all_examples)}, Train: {len(train_examples)}, Eval: {len(eval_examples)}\")\n",
        "        return (RankerDataset(train_examples, self.tokenizer, self.config.max_length),\n",
        "                RankerDataset(eval_examples, self.tokenizer, self.config.max_length))\n",
        "\n",
        "    def compute_metrics(self, eval_pred):\n",
        "        predictions, labels = eval_pred\n",
        "        scores = torch.sigmoid(torch.tensor(predictions)).numpy().flatten()\n",
        "        preds_binary = (scores > 0.5).astype(int)\n",
        "        labels = labels.flatten()\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds_binary, average='binary', zero_division=0)\n",
        "        return {'accuracy': accuracy_score(labels, preds_binary), 'precision': precision, 'recall': recall, 'f1': f1, 'auc': roc_auc_score(labels, scores)}\n",
        "\n",
        "    def train(self, train_dataset, eval_dataset):\n",
        "        os.makedirs(self.config.output_dir, exist_ok=True)\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=self.config.output_dir, num_train_epochs=self.config.num_epochs,\n",
        "            per_device_train_batch_size=self.config.batch_size, per_device_eval_batch_size=self.config.batch_size,\n",
        "            gradient_accumulation_steps=self.config.gradient_accumulation_steps,\n",
        "            learning_rate=self.config.learning_rate, weight_decay=self.config.weight_decay,\n",
        "            warmup_ratio=self.config.warmup_ratio, logging_steps=self.config.logging_steps,\n",
        "            save_steps=self.config.save_steps, eval_strategy=\"steps\", eval_steps=self.config.eval_steps,\n",
        "            save_strategy=\"steps\", load_best_model_at_end=self.config.load_best_model_at_end,\n",
        "            metric_for_best_model=self.config.metric_for_best_model,\n",
        "            greater_is_better=self.config.greater_is_better, fp16=self.config.fp16,\n",
        "            dataloader_num_workers=self.config.dataloader_num_workers,\n",
        "            max_grad_norm=self.config.max_grad_norm, report_to=\"none\"\n",
        "        )\n",
        "        trainer = WeightedTrainer(\n",
        "            model=self.model, args=training_args, train_dataset=train_dataset,\n",
        "            eval_dataset=eval_dataset, compute_metrics=self.compute_metrics,\n",
        "            callbacks=[EarlyStoppingCallback(early_stopping_patience=7)] # Patience of 5 is good for base training\n",
        "        )\n",
        "        logger.info(\"Starting training...\")\n",
        "        trainer.train()\n",
        "        trainer.save_model()\n",
        "        self.tokenizer.save_pretrained(self.config.output_dir)\n",
        "        with open(os.path.join(self.config.output_dir, \"training_config.json\"), 'w') as f:\n",
        "            json.dump(self.config.__dict__, f, indent=2)\n",
        "        logger.info(f\"Training completed. Model saved to {self.config.output_dir}\")\n",
        "        return trainer\n",
        "\n",
        "# ==============================================================================\n",
        "# --- MAIN EXECUTION BLOCK ---\n",
        "# ==============================================================================\n",
        "def main(model_name: str,\n",
        "         output_dir: str,\n",
        "         batch_size: int,\n",
        "         learning_rate: float,\n",
        "         num_epochs: int,\n",
        "         max_train_samples: Optional[int],\n",
        "         cache_file: str):\n",
        "    \"\"\"Main training function for notebook environments.\"\"\"\n",
        "    num_workers = max(1, mp.cpu_count() - 2)\n",
        "    config = TrainingConfig(\n",
        "        model_name=model_name, output_dir=output_dir, batch_size=batch_size,\n",
        "        learning_rate=learning_rate, num_epochs=num_epochs,\n",
        "        dataloader_num_workers=num_workers, embedding_batch_size=128,\n",
        "        ner_batch_size=64, hard_negative_ratio=0.1, weight_decay=0.05,\n",
        "        warmup_ratio=0.15\n",
        "    )\n",
        "    trainer_wrapper = NeuralRankerTrainer(config)\n",
        "    train_dataset, eval_dataset = trainer_wrapper.prepare_datasets(\n",
        "        dataset_name=\"TIGER-Lab/LongRAG\", subset=\"hotpot_qa\", train_split=\"full\",\n",
        "        max_train_samples=max_train_samples, cache_file=cache_file, num_workers=num_workers\n",
        "    )\n",
        "    trainer = trainer_wrapper.train(train_dataset, eval_dataset)\n",
        "    eval_results = trainer.evaluate()\n",
        "    logger.info(f\"Final evaluation metrics: {eval_results}\")\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TRAINING COMPLETED SUCCESSFULLY!\")\n",
        "    print(f\"Model saved to: {config.output_dir}\")\n",
        "    print(f\"Final F1 Score: {eval_results.get('eval_f1', 'N/A'):.4f}\")\n",
        "    print(f\"Final AUC Score: {eval_results.get('eval_auc', 'N/A'):.4f}\")\n",
        "    print(\"=\"*50)\n",
        "    return eval_results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"--- Starting STABLE Training for HotpotQA Base Model ---\")\n",
        "    main(\n",
        "        model_name=\"microsoft/deberta-v3-base\",\n",
        "        output_dir=\"./models/neural_ranker_hotpot_stable\",\n",
        "        max_train_samples=None, # Use all 7405 rows\n",
        "        cache_file=\"preprocessed_hotpotqa_stable.parquet\",\n",
        "        num_epochs=6,\n",
        "        batch_size=12,\n",
        "        learning_rate=1.5e-5\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4w1kW-4guKO1",
        "outputId": "7b7bb8e4-2fbf-4ebe-8f36-82a27e1a2409"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All old cache files have been deleted.\n",
            "--- Starting STABLE Training for HotpotQA Base Model ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cuda:0\n",
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6500' max='12072' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 6500/12072 52:16 < 44:49, 2.07 it/s, Epoch 3/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "      <th>Auc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.761200</td>\n",
              "      <td>0.730126</td>\n",
              "      <td>0.536979</td>\n",
              "      <td>0.518922</td>\n",
              "      <td>0.996764</td>\n",
              "      <td>0.682519</td>\n",
              "      <td>0.832330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.229000</td>\n",
              "      <td>0.183538</td>\n",
              "      <td>0.950777</td>\n",
              "      <td>0.967467</td>\n",
              "      <td>0.932786</td>\n",
              "      <td>0.949810</td>\n",
              "      <td>0.982030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.154600</td>\n",
              "      <td>0.175238</td>\n",
              "      <td>0.957862</td>\n",
              "      <td>0.977911</td>\n",
              "      <td>0.936769</td>\n",
              "      <td>0.956898</td>\n",
              "      <td>0.987680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.168300</td>\n",
              "      <td>0.127021</td>\n",
              "      <td>0.971535</td>\n",
              "      <td>0.965585</td>\n",
              "      <td>0.977844</td>\n",
              "      <td>0.971676</td>\n",
              "      <td>0.988273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>0.135000</td>\n",
              "      <td>0.137986</td>\n",
              "      <td>0.970044</td>\n",
              "      <td>0.974849</td>\n",
              "      <td>0.964899</td>\n",
              "      <td>0.969849</td>\n",
              "      <td>0.988677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.135800</td>\n",
              "      <td>0.114189</td>\n",
              "      <td>0.975513</td>\n",
              "      <td>0.969749</td>\n",
              "      <td>0.981578</td>\n",
              "      <td>0.975628</td>\n",
              "      <td>0.990237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1750</td>\n",
              "      <td>0.133600</td>\n",
              "      <td>0.111739</td>\n",
              "      <td>0.973400</td>\n",
              "      <td>0.967314</td>\n",
              "      <td>0.979836</td>\n",
              "      <td>0.973535</td>\n",
              "      <td>0.991668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.126500</td>\n",
              "      <td>0.088794</td>\n",
              "      <td>0.978247</td>\n",
              "      <td>0.971295</td>\n",
              "      <td>0.985561</td>\n",
              "      <td>0.978376</td>\n",
              "      <td>0.991994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2250</td>\n",
              "      <td>0.100400</td>\n",
              "      <td>0.113417</td>\n",
              "      <td>0.973897</td>\n",
              "      <td>0.973861</td>\n",
              "      <td>0.973861</td>\n",
              "      <td>0.973861</td>\n",
              "      <td>0.991692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.081000</td>\n",
              "      <td>0.101769</td>\n",
              "      <td>0.976259</td>\n",
              "      <td>0.964320</td>\n",
              "      <td>0.989047</td>\n",
              "      <td>0.976527</td>\n",
              "      <td>0.992270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2750</td>\n",
              "      <td>0.104600</td>\n",
              "      <td>0.109107</td>\n",
              "      <td>0.977750</td>\n",
              "      <td>0.968964</td>\n",
              "      <td>0.987055</td>\n",
              "      <td>0.977926</td>\n",
              "      <td>0.991258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.116100</td>\n",
              "      <td>0.093592</td>\n",
              "      <td>0.977626</td>\n",
              "      <td>0.974055</td>\n",
              "      <td>0.981329</td>\n",
              "      <td>0.977679</td>\n",
              "      <td>0.993081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3250</td>\n",
              "      <td>0.097800</td>\n",
              "      <td>0.097880</td>\n",
              "      <td>0.977999</td>\n",
              "      <td>0.966472</td>\n",
              "      <td>0.990291</td>\n",
              "      <td>0.978237</td>\n",
              "      <td>0.992440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.081700</td>\n",
              "      <td>0.091592</td>\n",
              "      <td>0.978620</td>\n",
              "      <td>0.966966</td>\n",
              "      <td>0.991038</td>\n",
              "      <td>0.978854</td>\n",
              "      <td>0.992902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3750</td>\n",
              "      <td>0.083000</td>\n",
              "      <td>0.090374</td>\n",
              "      <td>0.977502</td>\n",
              "      <td>0.980220</td>\n",
              "      <td>0.974608</td>\n",
              "      <td>0.977406</td>\n",
              "      <td>0.994130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.088600</td>\n",
              "      <td>0.076960</td>\n",
              "      <td>0.980112</td>\n",
              "      <td>0.971631</td>\n",
              "      <td>0.989047</td>\n",
              "      <td>0.980262</td>\n",
              "      <td>0.994514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4250</td>\n",
              "      <td>0.055800</td>\n",
              "      <td>0.091887</td>\n",
              "      <td>0.978620</td>\n",
              "      <td>0.964260</td>\n",
              "      <td>0.994025</td>\n",
              "      <td>0.978916</td>\n",
              "      <td>0.992008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.067900</td>\n",
              "      <td>0.105335</td>\n",
              "      <td>0.979739</td>\n",
              "      <td>0.979592</td>\n",
              "      <td>0.979836</td>\n",
              "      <td>0.979714</td>\n",
              "      <td>0.994611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4750</td>\n",
              "      <td>0.051400</td>\n",
              "      <td>0.078904</td>\n",
              "      <td>0.981231</td>\n",
              "      <td>0.972385</td>\n",
              "      <td>0.990540</td>\n",
              "      <td>0.981379</td>\n",
              "      <td>0.993181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.054800</td>\n",
              "      <td>0.094506</td>\n",
              "      <td>0.980609</td>\n",
              "      <td>0.976784</td>\n",
              "      <td>0.984566</td>\n",
              "      <td>0.980660</td>\n",
              "      <td>0.994113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5250</td>\n",
              "      <td>0.051300</td>\n",
              "      <td>0.089792</td>\n",
              "      <td>0.980982</td>\n",
              "      <td>0.971910</td>\n",
              "      <td>0.990540</td>\n",
              "      <td>0.981137</td>\n",
              "      <td>0.992156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.061800</td>\n",
              "      <td>0.093036</td>\n",
              "      <td>0.979863</td>\n",
              "      <td>0.978169</td>\n",
              "      <td>0.981578</td>\n",
              "      <td>0.979871</td>\n",
              "      <td>0.994574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5750</td>\n",
              "      <td>0.063200</td>\n",
              "      <td>0.106206</td>\n",
              "      <td>0.977750</td>\n",
              "      <td>0.979510</td>\n",
              "      <td>0.975853</td>\n",
              "      <td>0.977678</td>\n",
              "      <td>0.995032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.044700</td>\n",
              "      <td>0.096160</td>\n",
              "      <td>0.980733</td>\n",
              "      <td>0.972127</td>\n",
              "      <td>0.989793</td>\n",
              "      <td>0.980881</td>\n",
              "      <td>0.992458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6250</td>\n",
              "      <td>0.044100</td>\n",
              "      <td>0.103218</td>\n",
              "      <td>0.979242</td>\n",
              "      <td>0.977431</td>\n",
              "      <td>0.981080</td>\n",
              "      <td>0.979252</td>\n",
              "      <td>0.992890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.049600</td>\n",
              "      <td>0.104036</td>\n",
              "      <td>0.980733</td>\n",
              "      <td>0.977498</td>\n",
              "      <td>0.984068</td>\n",
              "      <td>0.980772</td>\n",
              "      <td>0.993320</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='671' max='671' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [671/671 00:28]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "TRAINING COMPLETED SUCCESSFULLY!\n",
            "Model saved to: ./models/neural_ranker_hotpot_stable\n",
            "Final F1 Score: 0.9814\n",
            "Final AUC Score: 0.9932\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "#!rm -f preprocessed_nq_full_dataset.parquet\n",
        "\"\"\"\n",
        "Neural Ranker Continual Finetuning Script\n",
        "Adapts a pre-trained HotpotQA neural ranker to work on Natural Questions (NQ) subset.\n",
        "This version includes a weighted loss function, tuned hyperparameters, and fixes for\n",
        "API argument mismatches.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import re\n",
        "import multiprocessing as mp\n",
        "from multiprocessing import Pool\n",
        "import shutil\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import json\n",
        "import logging\n",
        "from typing import List, Dict, Tuple, Optional, Any\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSequenceClassification,\n",
        "    TrainingArguments, Trainer, EarlyStoppingCallback,\n",
        "    pipeline as hf_pipeline\n",
        ")\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Ensure NLTK data is available\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ==============================================================================\n",
        "# --- CONFIGURATION ---\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class ContinualTrainingConfig:\n",
        "    model_name: str = \"./models/neural_ranker_hotpot_stable\"\n",
        "    max_length: int = 512\n",
        "    batch_size: int = 12\n",
        "    learning_rate: float = 2e-6\n",
        "    num_epochs: int = 4\n",
        "    warmup_ratio: float = 0.2\n",
        "    weight_decay: float = 0.05\n",
        "    output_dir: str = \"./models/neural_ranker_hotpot_nq\"\n",
        "    save_steps: int = 250\n",
        "    eval_steps: int = 250\n",
        "    logging_steps: int = 50\n",
        "    load_best_model_at_end: bool = True\n",
        "    metric_for_best_model: str = \"eval_f1\"\n",
        "    greater_is_better: bool = True\n",
        "    dataloader_num_workers: int = 2\n",
        "    fp16: bool = torch.cuda.is_available()\n",
        "    gradient_accumulation_steps: int = 3\n",
        "    max_grad_norm: float = 1.0\n",
        "    hard_negative_ratio: float = 0.1\n",
        "    min_sentence_length: int = 20\n",
        "    use_hop_features: bool = True\n",
        "    use_rhetorical_features: bool = True\n",
        "    use_entity_features: bool = True\n",
        "    use_reasoning_features: bool = True\n",
        "    embedding_batch_size: int = 128\n",
        "    ner_batch_size: int = 64\n",
        "\n",
        "@dataclass\n",
        "class SentenceWithMetadata:\n",
        "    text: str; doc_title: str; doc_id: int; sent_id: int; entities: List[str]\n",
        "    rhetorical_role: str = \"Background\"; hop_depth: int = 0\n",
        "    has_reasoning_signals: bool = False; is_causal: bool = False\n",
        "\n",
        "# ==============================================================================\n",
        "# --- ADAPTIVE DATA PROCESSOR ---\n",
        "# ==============================================================================\n",
        "class AdaptiveDataProcessor:\n",
        "    rhetorical_patterns = {\n",
        "        'Main Claim': ['argue', 'claim', 'assert', 'believe', 'conclude'],\n",
        "        'Supporting Evidence': ['evidence', 'data', 'research', 'study', 'found'],\n",
        "        'Expert Opinion': ['expert', 'according to', 'stated', 'opinion'],\n",
        "    }\n",
        "    def __init__(self, config: ContinualTrainingConfig, sbert_model: SentenceTransformer):\n",
        "        self.config = config\n",
        "        self.sbert_model = sbert_model\n",
        "        if self.config.use_entity_features:\n",
        "            self.ner_pipeline = hf_pipeline(\"ner\", model=\"dslim/bert-base-NER\", device=0 if torch.cuda.is_available() else -1, grouped_entities=True)\n",
        "        else:\n",
        "            self.ner_pipeline = None\n",
        "    @staticmethod\n",
        "    def _adaptive_parse_worker(row: Dict) -> Optional[Dict]:\n",
        "        try:\n",
        "            query = row.get('query')\n",
        "            if not isinstance(query, str) or not query.strip(): return None\n",
        "            answer_text = \"\"\n",
        "            if row.get('answer') and isinstance(row['answer'], list) and row['answer']:\n",
        "                answer_text = row['answer'][0].lower().strip()\n",
        "            sp_titles = set(row.get('sp', []))\n",
        "            if not answer_text and not sp_titles: return None\n",
        "            raw_context = row.get('context', \"\")\n",
        "            if not isinstance(raw_context, str) or not raw_context.strip(): return None\n",
        "            docs_raw = re.split(r'\\n\\nTitle: ', raw_context)\n",
        "            sentences = []\n",
        "            if docs_raw and docs_raw[0].strip():\n",
        "                first_doc = docs_raw[0].strip()\n",
        "                if not first_doc.startswith(\"Title:\"): docs_raw[0] = \"Title: \" + first_doc\n",
        "            positive_count = 0\n",
        "            for doc_part in docs_raw:\n",
        "                if not doc_part or not doc_part.strip(): continue\n",
        "                lines = doc_part.split('\\n', 1)\n",
        "                doc_title = lines[0].replace(\"Title: \", \"\").strip()\n",
        "                doc_text = lines[1].strip() if len(lines) > 1 else \"\"\n",
        "                for sent_text in sent_tokenize(doc_text):\n",
        "                    if len(sent_text) <= ContinualTrainingConfig.min_sentence_length: continue\n",
        "                    is_positive = (sp_titles and doc_title in sp_titles) or (answer_text and answer_text in sent_text.lower())\n",
        "                    if is_positive: positive_count += 1\n",
        "                    sentences.append({\"text\": sent_text, \"doc_title\": doc_title, \"is_positive_source\": is_positive})\n",
        "            return {\"query\": query, \"sentences\": sentences} if positive_count > 0 else None\n",
        "        except Exception:\n",
        "            return None\n",
        "    def _batch_process_sentences(self, all_sentences: List[Dict], chunk_size: int = 20000):\n",
        "        sentence_texts = [s['text'] for s in all_sentences]\n",
        "        embeddings_list = []\n",
        "        try:\n",
        "            for i in tqdm(range(0, len(sentence_texts), chunk_size), desc=\"SBERT Encoding\"):\n",
        "                chunk = sentence_texts[i:i + chunk_size]\n",
        "                embeddings_list.append(self.sbert_model.encode(chunk, batch_size=self.config.embedding_batch_size, show_progress_bar=False, convert_to_numpy=True))\n",
        "            sentence_embeddings = np.vstack(embeddings_list) if embeddings_list else np.zeros((0, self.sbert_model.get_sentence_embedding_dimension()))\n",
        "        except Exception as e:\n",
        "            logger.error(f\"SBERT encoding failed: {e}\")\n",
        "            sentence_embeddings = np.zeros((len(sentence_texts), self.sbert_model.get_sentence_embedding_dimension()))\n",
        "        all_entities = [[] for _ in sentence_texts]\n",
        "        if self.ner_pipeline:\n",
        "            for i in tqdm(range(0, len(sentence_texts), chunk_size), desc=\"Transformer NER\"):\n",
        "                chunk_texts = sentence_texts[i:i + chunk_size]\n",
        "                try:\n",
        "                    results = self.ner_pipeline(chunk_texts, batch_size=self.config.ner_batch_size)\n",
        "                    for j, ner_result in enumerate(results):\n",
        "                        all_entities[i + j] = [(entity['word'], entity['entity_group']) for entity in ner_result]\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"NER chunk failed: {e}\")\n",
        "        return sentence_embeddings, all_entities\n",
        "    @staticmethod\n",
        "    def _assemble_examples_worker(args: Tuple) -> List[Tuple[str, int, Dict]]:\n",
        "        parsed_row, query_embedding, sentence_data_map, config, embedding_dim = args\n",
        "        query, sentences = parsed_row['query'], parsed_row['sentences']\n",
        "        if not sentences: return []\n",
        "        def _classify_rhetorical_role(text):\n",
        "            text_lower = text.lower()\n",
        "            for role, patterns in AdaptiveDataProcessor.rhetorical_patterns.items():\n",
        "                if any(p in text_lower for p in patterns): return role\n",
        "            return \"Background_Information\"\n",
        "        def _detect_causal_language(text):\n",
        "            return any(word in text.lower() for word in ['cause', 'because', 'due to', 'result', 'lead to'])\n",
        "        def _create_structured_input(q, s_meta):\n",
        "            components = []\n",
        "            if config.use_hop_features: components.append(f\"[HOP:{s_meta.hop_depth}]\")\n",
        "            if config.use_rhetorical_features: components.append(f\"[ROLE:{s_meta.rhetorical_role.replace(' ', '_')}]\")\n",
        "            if config.use_entity_features and s_meta.entities: components.append(f\"[ENT:{','.join(s_meta.entities[:3])}]\")\n",
        "            if config.use_reasoning_features:\n",
        "                if s_meta.is_causal: components.append(\"[CAUSAL]\")\n",
        "                if s_meta.has_reasoning_signals: components.append(\"[REASONING]\")\n",
        "            return f\"{' '.join(components)} {q} [SEP] {s_meta.text}\"\n",
        "        positives, negatives = [], []\n",
        "        sent_emb_list = [sentence_data_map.get(s['text'], {}).get('embedding', np.zeros(embedding_dim)) for s in sentences]\n",
        "        sentence_embeddings = np.array(sent_emb_list)\n",
        "        q_norm = np.linalg.norm(query_embedding) + 1e-9\n",
        "        s_norms = np.linalg.norm(sentence_embeddings, axis=1) + 1e-9\n",
        "        similarities = (sentence_embeddings @ query_embedding) / (s_norms * q_norm)\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            entry = sentence_data_map.get(sentence['text'], {})\n",
        "            entities = entry.get('entities', [])\n",
        "            is_causal = _detect_causal_language(sentence['text'])\n",
        "            sentence_meta = SentenceWithMetadata(text=sentence['text'], doc_title=sentence.get('doc_title', ''), entities=[e[0] for e in entities], rhetorical_role=_classify_rhetorical_role(sentence['text']), is_causal=is_causal, has_reasoning_signals=(len(entities) > 1 or is_causal), doc_id=0, sent_id=i)\n",
        "            structured_input = _create_structured_input(query, sentence_meta)\n",
        "            metadata = {'similarity': float(similarities[i]), 'doc_title': sentence.get('doc_title', '')}\n",
        "            example = (structured_input, int(sentence.get('is_positive_source', False)), metadata)\n",
        "            if sentence.get('is_positive_source', False): positives.append(example)\n",
        "            else: negatives.append(example)\n",
        "        if not positives: return []\n",
        "        negatives.sort(key=lambda x: x[2]['similarity'], reverse=True)\n",
        "        num_hard = int(len(positives) * config.hard_negative_ratio)\n",
        "        selected_negatives = negatives[:num_hard]\n",
        "        if len(positives) - num_hard > 0 and len(negatives) > num_hard:\n",
        "             tail = negatives[num_hard:]\n",
        "             num_easy = len(positives) - num_hard\n",
        "             indices = np.linspace(0, len(tail) - 1, num_easy).astype(int)\n",
        "             selected_negatives.extend([tail[idx] for idx in indices])\n",
        "        return positives + selected_negatives\n",
        "    def process_dataset(self, dataset: List[Dict], num_workers: int, cache_prefix: str) -> List[Tuple[str, int, Dict]]:\n",
        "        cache_dir = f\"stage3_cache_{cache_prefix}\"\n",
        "        os.makedirs(cache_dir, exist_ok=True)\n",
        "        cache_files = {\"parsed_rows.json\", \"sentence_embeddings.npy\", \"sentence_texts.json\", \"sentence_meta.json\", \"query_embeddings.npy\", \"query_texts.json\"}\n",
        "        if all(os.path.exists(os.path.join(cache_dir, f)) for f in cache_files):\n",
        "            logger.info(f\"Loading {cache_prefix.upper()} cache...\")\n",
        "            with open(os.path.join(cache_dir, \"parsed_rows.json\"), \"r\") as f: parsed_rows = json.load(f)\n",
        "            sentence_embeddings = np.load(os.path.join(cache_dir, \"sentence_embeddings.npy\"))\n",
        "            with open(os.path.join(cache_dir, \"sentence_texts.json\"), \"r\") as f: sentence_texts = json.load(f)\n",
        "            with open(os.path.join(cache_dir, \"sentence_meta.json\"), \"r\") as f: sentence_meta = json.load(f)\n",
        "            query_embeddings = np.load(os.path.join(cache_dir, \"query_embeddings.npy\"))\n",
        "            with open(os.path.join(cache_dir, \"query_texts.json\"), \"r\") as f: query_texts = json.load(f)\n",
        "            query_embedding_map = {q: emb for q, emb in zip(query_texts, query_embeddings)}\n",
        "            sentence_data_map = {text: {\"embedding\": sentence_embeddings[i], \"entities\": meta.get(\"entities\", [])} for i, (text, meta) in enumerate(zip(sentence_texts, sentence_meta))}\n",
        "        else:\n",
        "            logger.info(f\"No complete {cache_prefix.upper()} cache found. Running full preprocessing...\")\n",
        "            with Pool(processes=num_workers) as pool:\n",
        "                parsed_rows = list(tqdm(pool.imap(self._adaptive_parse_worker, dataset), total=len(dataset), desc=f\"Stage 1: Parsing {cache_prefix.upper()}\"))\n",
        "            parsed_rows = [row for row in parsed_rows if row]\n",
        "            if not parsed_rows: raise ValueError(f\"Stage 1 parsing resulted in zero processable rows.\")\n",
        "            unique_sentences = {s['text']: s for row in parsed_rows for s in row['sentences']}\n",
        "            unique_queries = sorted(list({row['query'] for row in parsed_rows}))\n",
        "            all_unique_sentences = list(unique_sentences.values())\n",
        "            sentence_embeddings, all_entities = self._batch_process_sentences(all_unique_sentences)\n",
        "            query_embeddings = self.sbert_model.encode([f\"query: {q}\" for q in unique_queries], batch_size=self.config.embedding_batch_size, show_progress_bar=True, convert_to_numpy=True)\n",
        "            query_embedding_map = {q: emb for q, emb in zip(unique_queries, query_embeddings)}\n",
        "            sentence_texts = [s['text'] for s in all_unique_sentences]\n",
        "            sentence_meta = [{\"entities\": all_entities[i]} for i in range(len(all_entities))]\n",
        "            sentence_data_map = {text: {\"embedding\": sentence_embeddings[i], \"entities\": sentence_meta[i][\"entities\"]} for i, text in enumerate(sentence_texts)}\n",
        "            temp_files = {f: f\"{f}.tmp\" for f in cache_files}\n",
        "            try:\n",
        "                with open(os.path.join(cache_dir, temp_files[\"parsed_rows.json\"]), \"w\") as f: json.dump(parsed_rows, f)\n",
        "                np.save(os.path.join(cache_dir, temp_files[\"sentence_embeddings.npy\"]), sentence_embeddings)\n",
        "                with open(os.path.join(cache_dir, temp_files[\"sentence_texts.json\"]), \"w\") as f: json.dump(sentence_texts, f)\n",
        "                with open(os.path.join(cache_dir, temp_files[\"sentence_meta.json\"]), \"w\") as f: json.dump(sentence_meta, f)\n",
        "                np.save(os.path.join(cache_dir, temp_files[\"query_embeddings.npy\"]), query_embeddings)\n",
        "                with open(os.path.join(cache_dir, temp_files[\"query_texts.json\"]), \"w\") as f: json.dump(unique_queries, f)\n",
        "                for final, temp in temp_files.items():\n",
        "                    shutil.move(os.path.join(cache_dir, temp), os.path.join(cache_dir, final))\n",
        "                logger.info(f\"Saved {cache_prefix.upper()} cache to '{cache_dir}'.\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Failed to write cache files: {e}. Cleaning up.\")\n",
        "                for temp in temp_files.values():\n",
        "                    if os.path.exists(os.path.join(cache_dir, temp)): os.remove(os.path.join(cache_dir, temp))\n",
        "        all_examples = []\n",
        "        assembly_args = [(row, query_embedding_map.get(row['query']), sentence_data_map, self.config, sentence_embeddings.shape[1]) for row in parsed_rows if query_embedding_map.get(row['query']) is not None]\n",
        "        with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "            futures = [executor.submit(self._assemble_examples_worker, arg) for arg in assembly_args]\n",
        "            for future in tqdm(as_completed(futures), total=len(futures), desc=f\"Stage 4: Assembling {cache_prefix.upper()}\"):\n",
        "                all_examples.extend(future.result())\n",
        "        return all_examples\n",
        "\n",
        "# ==============================================================================\n",
        "# --- DATASET & TRAINER CLASSES ---\n",
        "# ==============================================================================\n",
        "class RankerDataset(Dataset):\n",
        "    def __init__(self, examples: List[Tuple[str, int, Dict]], tokenizer, max_length: int):\n",
        "        self.examples = examples; self.tokenizer = tokenizer; self.max_length = max_length\n",
        "    def __len__(self): return len(self.examples)\n",
        "    def __getitem__(self, idx):\n",
        "        text, label, _ = self.examples[idx]\n",
        "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
        "        return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'labels': torch.tensor(label, dtype=torch.float)}\n",
        "\n",
        "class WeightedTrainer(Trainer):\n",
        "    \"\"\"Custom Trainer to apply a weight to the positive class in the loss function.\"\"\"\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "        # This weight penalizes the model more for misclassifying a \"0\" than a \"1\".\n",
        "        # It forces the model to learn to say \"no\" and breaks the \"always predict positive\" habit.\n",
        "        pos_weight = torch.tensor([1.2], device=model.device) # 20% more penalty for false negatives\n",
        "        loss_fct = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "        loss = loss_fct(logits.view(-1), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "class ContinualNeuralRankerTrainer:\n",
        "    def __init__(self, config: ContinualTrainingConfig):\n",
        "        self.config = config\n",
        "        self.sbert_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.data_processor = AdaptiveDataProcessor(config, self.sbert_model)\n",
        "        logger.info(f\"Loading pre-trained model from: {config.model_name}\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(config.model_name, num_labels=1, problem_type=\"regression\")\n",
        "        special_tokens = ['[HOP:0]', '[HOP:1]', '[ROLE:Main_Claim]', '[ROLE:Supporting_Evidence]', '[ROLE:Expert_Opinion]', '[ROLE:Background_Information]', '[ENT:', '[CAUSAL]', '[REASONING]']\n",
        "        new_tokens = [token for token in special_tokens if token not in self.tokenizer.vocab]\n",
        "        if new_tokens:\n",
        "            self.tokenizer.add_special_tokens({'additional_special_tokens': new_tokens})\n",
        "            self.model.resize_token_embeddings(len(self.tokenizer))\n",
        "\n",
        "    def prepare_datasets(self, dataset_name: str, subset: str, train_split: str, max_train_samples: int, cache_file: str, num_workers: int):\n",
        "        if os.path.exists(cache_file):\n",
        "            logger.info(f\"Loading preprocessed dataset from {cache_file}...\")\n",
        "            df = pd.read_parquet(cache_file)\n",
        "            all_examples = [(row['text'], int(row['label']), json.loads(row['metadata'])) for _, row in df.iterrows()]\n",
        "        else:\n",
        "            logger.info(f\"Loading dataset: {dataset_name}, subset: {subset}\")\n",
        "            dataset = load_dataset(dataset_name, name=subset, split=train_split)\n",
        "            if max_train_samples:\n",
        "                dataset = dataset.select(range(min(max_train_samples, len(dataset))))\n",
        "            all_examples = self.data_processor.process_dataset(list(dataset), num_workers=num_workers, cache_prefix=subset)\n",
        "            if not all_examples: raise RuntimeError(\"Data processing yielded zero examples. Aborting.\")\n",
        "            df_to_cache = pd.DataFrame([{\"text\": t, \"label\": l, \"metadata\": json.dumps(m)} for t, l, m in all_examples])\n",
        "            df_to_cache.to_parquet(cache_file, index=False)\n",
        "            logger.info(f\"Saved preprocessed data to {cache_file}\")\n",
        "        np.random.shuffle(all_examples)\n",
        "        split_point = int(0.9 * len(all_examples))\n",
        "        train_examples, eval_examples = all_examples[:split_point], all_examples[split_point:]\n",
        "        logger.info(f\"Total examples: {len(all_examples)}, Train: {len(train_examples)}, Eval: {len(eval_examples)}\")\n",
        "        return (RankerDataset(train_examples, self.tokenizer, self.config.max_length),\n",
        "                RankerDataset(eval_examples, self.tokenizer, self.config.max_length))\n",
        "\n",
        "    def compute_metrics(self, eval_pred):\n",
        "        predictions, labels = eval_pred\n",
        "        scores = torch.sigmoid(torch.tensor(predictions)).numpy().flatten()\n",
        "        preds_binary = (scores > 0.5).astype(int)\n",
        "        labels = labels.flatten()\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds_binary, average='binary', zero_division=0)\n",
        "        return {'accuracy': accuracy_score(labels, preds_binary), 'precision': precision, 'recall': recall, 'f1': f1, 'auc': roc_auc_score(labels, scores)}\n",
        "\n",
        "    def train(self, train_dataset, eval_dataset):\n",
        "        os.makedirs(self.config.output_dir, exist_ok=True)\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=self.config.output_dir,\n",
        "            num_train_epochs=self.config.num_epochs,\n",
        "            per_device_train_batch_size=self.config.batch_size,\n",
        "            per_device_eval_batch_size=self.config.batch_size,\n",
        "            gradient_accumulation_steps=self.config.gradient_accumulation_steps,\n",
        "            learning_rate=self.config.learning_rate,\n",
        "            weight_decay=self.config.weight_decay,\n",
        "            warmup_ratio=self.config.warmup_ratio,\n",
        "            logging_steps=self.config.logging_steps,\n",
        "            save_steps=self.config.save_steps,\n",
        "            eval_strategy=\"steps\",\n",
        "            eval_steps=self.config.eval_steps,\n",
        "            save_strategy=\"steps\",\n",
        "            load_best_model_at_end=self.config.load_best_model_at_end,\n",
        "            metric_for_best_model=self.config.metric_for_best_model,\n",
        "            greater_is_better=self.config.greater_is_better,\n",
        "            fp16=self.config.fp16,\n",
        "            dataloader_num_workers=self.config.dataloader_num_workers,\n",
        "            max_grad_norm=self.config.max_grad_norm,\n",
        "            report_to=\"none\"\n",
        "        )\n",
        "        trainer = WeightedTrainer(\n",
        "            model=self.model, args=training_args, train_dataset=train_dataset,\n",
        "            eval_dataset=eval_dataset, compute_metrics=self.compute_metrics,\n",
        "            callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
        "        )\n",
        "        logger.info(\"Starting continual finetuning...\")\n",
        "        trainer.train()\n",
        "        trainer.save_model()\n",
        "        self.tokenizer.save_pretrained(self.config.output_dir)\n",
        "        with open(os.path.join(self.config.output_dir, \"continual_training_config.json\"), 'w') as f:\n",
        "            json.dump(self.config.__dict__, f, indent=2)\n",
        "        logger.info(f\"Continual finetuning completed. Model saved to {self.config.output_dir}\")\n",
        "        return trainer\n",
        "\n",
        "# ==============================================================================\n",
        "# --- MAIN EXECUTION FUNCTION ---\n",
        "# ==============================================================================\n",
        "def continual_finetune(pretrained_model_path: str,\n",
        "                      output_dir: str,\n",
        "                      dataset_subset: str,\n",
        "                      batch_size: int,\n",
        "                      learning_rate: float,\n",
        "                      num_epochs: int,\n",
        "                      max_train_samples: Optional[int],\n",
        "                      cache_file: str,\n",
        "                      gradient_accumulation_steps: int):\n",
        "\n",
        "    logger.info(\"=\"*80)\n",
        "    logger.info(f\"STARTING CONTINUAL FINETUNING ON '{dataset_subset.upper()}' SUBSET\")\n",
        "    logger.info(\"=\"*80)\n",
        "\n",
        "    num_workers = max(1, mp.cpu_count() - 2)\n",
        "    config = ContinualTrainingConfig(\n",
        "        model_name=pretrained_model_path,\n",
        "        output_dir=output_dir,\n",
        "        batch_size=batch_size,\n",
        "        learning_rate=learning_rate,\n",
        "        num_epochs=num_epochs,\n",
        "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "        dataloader_num_workers=num_workers,\n",
        "        embedding_batch_size=128,\n",
        "        ner_batch_size=64\n",
        "    )\n",
        "\n",
        "    trainer_wrapper = ContinualNeuralRankerTrainer(config)\n",
        "\n",
        "    train_dataset, eval_dataset = trainer_wrapper.prepare_datasets(\n",
        "        dataset_name=\"TIGER-Lab/LongRAG\",\n",
        "        subset=dataset_subset,\n",
        "        train_split=\"full\",\n",
        "        max_train_samples=max_train_samples,\n",
        "        cache_file=cache_file,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    trainer = trainer_wrapper.train(train_dataset, eval_dataset)\n",
        "    eval_results = trainer.evaluate()\n",
        "    logger.info(f\"Final evaluation metrics on {dataset_subset.upper()}: {eval_results}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"CONTINUAL FINETUNING COMPLETED SUCCESSFULLY!\")\n",
        "    print(f\"Original model: {pretrained_model_path}\")\n",
        "    print(f\"Enhanced model saved to: {config.output_dir}\")\n",
        "    print(f\"Final F1 Score on {dataset_subset.upper()}: {eval_results.get('eval_f1', 'N/A'):.4f}\")\n",
        "    print(f\"Final AUC Score on {dataset_subset.upper()}: {eval_results.get('eval_auc', 'N/A'):.4f}\")\n",
        "    print(\"Your neural ranker now handles both complex reasoning AND factual questions!\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    return eval_results\n",
        "\n",
        "# ==============================================================================\n",
        "# --- MAIN EXECUTION BLOCK ---\n",
        "# ==============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    continual_finetune(\n",
        "        pretrained_model_path=\"./models/neural_ranker_hotpot_stable\", # The successfully trained HotpotQA model\n",
        "        output_dir=\"./models/neural_ranker_hotpot_nq_final3\",\n",
        "        dataset_subset=\"nq\",\n",
        "        max_train_samples=None,  # Use the entire NQ dataset\n",
        "        cache_file=\"preprocessed_nq_full_dataset.parquet\",\n",
        "        num_epochs=2,\n",
        "        batch_size=12,\n",
        "        gradient_accumulation_steps=3,\n",
        "        learning_rate=2e-6\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 940
        },
        "id": "fntp3zzKj5sJ",
        "outputId": "c2652fd5-199c-4ed8-a40d-0019e856c644"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cuda:0\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/pipelines/token_classification.py:186: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4568' max='4568' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4568/4568 37:58, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "      <th>Auc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.684400</td>\n",
              "      <td>0.647508</td>\n",
              "      <td>0.804555</td>\n",
              "      <td>0.761579</td>\n",
              "      <td>0.883506</td>\n",
              "      <td>0.818024</td>\n",
              "      <td>0.871173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.451100</td>\n",
              "      <td>0.436985</td>\n",
              "      <td>0.834447</td>\n",
              "      <td>0.793565</td>\n",
              "      <td>0.901564</td>\n",
              "      <td>0.844124</td>\n",
              "      <td>0.898016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.407900</td>\n",
              "      <td>0.384410</td>\n",
              "      <td>0.858207</td>\n",
              "      <td>0.820498</td>\n",
              "      <td>0.914997</td>\n",
              "      <td>0.865174</td>\n",
              "      <td>0.918643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.352500</td>\n",
              "      <td>0.348749</td>\n",
              "      <td>0.872112</td>\n",
              "      <td>0.834822</td>\n",
              "      <td>0.926007</td>\n",
              "      <td>0.878054</td>\n",
              "      <td>0.931231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>0.356200</td>\n",
              "      <td>0.319746</td>\n",
              "      <td>0.883609</td>\n",
              "      <td>0.851171</td>\n",
              "      <td>0.928210</td>\n",
              "      <td>0.888023</td>\n",
              "      <td>0.940842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.337000</td>\n",
              "      <td>0.305844</td>\n",
              "      <td>0.888974</td>\n",
              "      <td>0.850248</td>\n",
              "      <td>0.942744</td>\n",
              "      <td>0.894110</td>\n",
              "      <td>0.945165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1750</td>\n",
              "      <td>0.331600</td>\n",
              "      <td>0.292743</td>\n",
              "      <td>0.892916</td>\n",
              "      <td>0.847881</td>\n",
              "      <td>0.956177</td>\n",
              "      <td>0.898779</td>\n",
              "      <td>0.949260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.329200</td>\n",
              "      <td>0.276657</td>\n",
              "      <td>0.901237</td>\n",
              "      <td>0.865582</td>\n",
              "      <td>0.948690</td>\n",
              "      <td>0.905232</td>\n",
              "      <td>0.954524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2250</td>\n",
              "      <td>0.277900</td>\n",
              "      <td>0.271594</td>\n",
              "      <td>0.904413</td>\n",
              "      <td>0.881765</td>\n",
              "      <td>0.932834</td>\n",
              "      <td>0.906581</td>\n",
              "      <td>0.958086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.276700</td>\n",
              "      <td>0.265102</td>\n",
              "      <td>0.905836</td>\n",
              "      <td>0.869801</td>\n",
              "      <td>0.953314</td>\n",
              "      <td>0.909645</td>\n",
              "      <td>0.958486</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2750</td>\n",
              "      <td>0.301200</td>\n",
              "      <td>0.262495</td>\n",
              "      <td>0.907697</td>\n",
              "      <td>0.871883</td>\n",
              "      <td>0.954636</td>\n",
              "      <td>0.911384</td>\n",
              "      <td>0.958609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.254800</td>\n",
              "      <td>0.256281</td>\n",
              "      <td>0.910325</td>\n",
              "      <td>0.876416</td>\n",
              "      <td>0.954195</td>\n",
              "      <td>0.913653</td>\n",
              "      <td>0.960998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3250</td>\n",
              "      <td>0.301500</td>\n",
              "      <td>0.253268</td>\n",
              "      <td>0.912515</td>\n",
              "      <td>0.884663</td>\n",
              "      <td>0.947589</td>\n",
              "      <td>0.915045</td>\n",
              "      <td>0.962024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.264600</td>\n",
              "      <td>0.249666</td>\n",
              "      <td>0.911420</td>\n",
              "      <td>0.878499</td>\n",
              "      <td>0.953755</td>\n",
              "      <td>0.914581</td>\n",
              "      <td>0.962703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3750</td>\n",
              "      <td>0.253000</td>\n",
              "      <td>0.247330</td>\n",
              "      <td>0.913500</td>\n",
              "      <td>0.883773</td>\n",
              "      <td>0.951112</td>\n",
              "      <td>0.916207</td>\n",
              "      <td>0.963845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.220800</td>\n",
              "      <td>0.248495</td>\n",
              "      <td>0.914486</td>\n",
              "      <td>0.882736</td>\n",
              "      <td>0.954856</td>\n",
              "      <td>0.917381</td>\n",
              "      <td>0.963386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4250</td>\n",
              "      <td>0.275100</td>\n",
              "      <td>0.246498</td>\n",
              "      <td>0.913610</td>\n",
              "      <td>0.880991</td>\n",
              "      <td>0.955296</td>\n",
              "      <td>0.916640</td>\n",
              "      <td>0.963618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.252600</td>\n",
              "      <td>0.244894</td>\n",
              "      <td>0.915471</td>\n",
              "      <td>0.885300</td>\n",
              "      <td>0.953534</td>\n",
              "      <td>0.918151</td>\n",
              "      <td>0.964429</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='762' max='762' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [762/762 00:32]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "CONTINUAL FINETUNING COMPLETED SUCCESSFULLY!\n",
            "Original model: ./models/neural_ranker_hotpot_stable\n",
            "Enhanced model saved to: ./models/neural_ranker_hotpot_nq_final3\n",
            "Final F1 Score on NQ: 0.9182\n",
            "Final AUC Score on NQ: 0.9644\n",
            "Your neural ranker now handles both complex reasoning AND factual questions!\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}